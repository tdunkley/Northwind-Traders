{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student Name: Troy D. Dunkley\n",
    "* Student Pace: Full Time\n",
    "* Scheduled Project Review Date/Time: Monday 10.28.19\n",
    "* Instructors' Names: Amber Yandow and Howard Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Industry Standard Proces for Data Mining, also known as the CRISP-DM Methodology, is being employed for this project. It is an open standard process model that describes common approaches used by data mining experts. CRISP-DM is currently the dominant process framework for data mining. It is comprised of the following phases:\n",
    "\n",
    "* Business Understanding\n",
    "* Data Understanding\n",
    "* Data Preparation\n",
    "* Modeling\n",
    "* Evaluation\n",
    "* Deployment\n",
    "\n",
    "Below is a diagram of the methodology process flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CRISPDM_Process_Diagram.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Statistical Analyses and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Business Understanding Phase focuses on understanding the project objectives and requirements from a business perspective, and then converting this knowledge into a data mining problem definition and a preliminary plan. The primary tasks within this phase include the following:\n",
    "\n",
    "* Determine Business Objectives\n",
    "* Assess Situation\n",
    "* Determine Data Mining Goals\n",
    "* Produce Project Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Business Objective\n",
    "\n",
    "*Background:*\n",
    "\n",
    "The Northwind database is a sample database that was originally created by Microsoft and used as the basis for their tutorials in a variety of database products for decades. The Northwind database contains the sales data for a fictitious company called ‚ÄúNorthwind Traders,‚Äù which imports and exports specialty foods from around the world. The Northwind database contains schema for a small-business ERP, with customers, orders, inventory, purchasing, suppliers, shipping, employees, and single-entry accounting\n",
    "\n",
    "*Business goals:* \n",
    "\n",
    "The goal of your project is to query the database to get the data needed to perform a statistical analysis. In this statistical analysis, you'll need to perform a hypothesis test (or perhaps several) to answer the following questions:\n",
    "\n",
    "* Does discount amount have a statistically significant effect on the quantity of a product in an order? If so, at what level(s) of discount?\n",
    "\n",
    "* Is there a statistically significant difference in discount between Categories?\n",
    "\n",
    "* Is there a statistically significant difference in performance of Shipping Companies?\n",
    "\n",
    "* Is there a statistically significant difference in performance of Suppliers?\n",
    "\n",
    "\n",
    "*Business success criteria:* \n",
    "\n",
    "To use Hypothesis Testing, Statistical power and ANOVA testing to design experiments rigorously and interpret them thoughtfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Situation\n",
    "This task is not applicable for this project; however, assessments are typically comprised of the following:\n",
    "\n",
    "Prodiving an inventory of resources (Data Managers, Technical Support, etc.)\n",
    "Document requirements, assumptions and constraints\n",
    "Identify risks and contingencies\n",
    "Chronicle relevant terminology\n",
    "Preparation of Cost-Benefit Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Data Mining Goals\n",
    "The Data Mining goal for this project is to better understand hypothesis testing and t-tests by examining the concept of power; an idea closely related to type II errors. With that, it should be determined how the rate of type I errors, power, sample size and effect size are intrinsically related to one another. From there, the concept of ANOVA - Analysis of Variance - allows for multiple A/B tests to be conducted simultaneously, testing for the influence of multiple factors all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Project Plan\n",
    "This task is not applicable for this project, however we will leveraging various Python librairies to assist us with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase starts with an initial data collection and proceeds with activities in order to get familiar with the data, to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. Below are tasks associated with this phase:\n",
    "\n",
    "* Collect Initial Data\n",
    "* Describe Data\n",
    "* Explore Data\n",
    "* Verify Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn')\n",
    "# import folium as fl\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm  # anova\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sqlite3\n",
    "from statsmodels.formula.api import ols\n",
    "from IPython.display import display\n",
    "\n",
    "# read data into dataframe:\n",
    "\n",
    "conn = sqlite3.connect('Northwind_Small.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to put all tables into pandas dataframes\n",
    "\n",
    "tables = cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "tables = [i[0] for i in tables]\n",
    "\n",
    "# dfs = []\n",
    "# for i in tables:\n",
    "#     table = cur.execute('SELECT * FROM \"'+i+'\"').fetchall()\n",
    "#     columns = cur.execute('PRAGMA table_info(\"'+i+'\")').fetchall()\n",
    "#     df = pd.DataFrame(table, columns=[i[1] for i in columns])\n",
    "#     # Cute little function to make a string into variable name\n",
    "#     foo = \"df\"+i   # df concatenate TableName\n",
    "# #     exec(foo+\".head()\") # => \n",
    "#     foo = pd.DataFrame(table, columns=[i[1] for i in columns])\n",
    "#     # Keep all dataframe names in the list to remember what we have\n",
    "#     dfs.append(foo)\n",
    "\n",
    "    # Loop to put all tables into pandas dataframes\n",
    "dfs = []\n",
    "for i in tables:\n",
    "    table = cur.execute('SELECT * FROM \"'+i+'\"').fetchall()\n",
    "    columns = cur.execute('PRAGMA table_info(\"'+i+'\")').fetchall()\n",
    "    df = pd.DataFrame(table, columns=[i[1] for i in columns])\n",
    "    # Cute little function to make a string into variable name\n",
    "#     foo = \"df\"+i+'.head()'\n",
    "    foo = \"df\"+i\n",
    "    exec(foo+\"=df\") # => TableName_df\n",
    "    # Keep all dataframe names in the list to remember what we have\n",
    "    dfs.append(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dfEmployee',\n",
       " 'dfCategory',\n",
       " 'dfCustomer',\n",
       " 'dfShipper',\n",
       " 'dfSupplier',\n",
       " 'dfOrder',\n",
       " 'dfProduct',\n",
       " 'dfOrderDetail',\n",
       " 'dfCustomerCustomerDemo',\n",
       " 'dfCustomerDemographic',\n",
       " 'dfRegion',\n",
       " 'dfTerritory',\n",
       " 'dfEmployeeTerritory']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs\n",
    "# display(dfEmployee.head())\n",
    "# display(dfCategory.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfEmployee.head()\n",
      "dfCategory.head()\n",
      "dfCustomer.head()\n",
      "dfShipper.head()\n",
      "dfSupplier.head()\n",
      "dfOrder.head()\n",
      "dfProduct.head()\n",
      "dfOrderDetail.head()\n",
      "dfCustomerCustomerDemo.head()\n",
      "dfCustomerDemographic.head()\n",
      "dfRegion.head()\n",
      "dfTerritory.head()\n",
      "dfEmployeeTerritory.head()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    ">>> def foo():\n",
    "    for i in dfs:\n",
    "#         return i+\".head()\"\n",
    "        print (i+\".head()\")\n",
    ">>> display(foo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dfs\n",
    "# tables\n",
    "# tables = []\n",
    "\n",
    "# def tbl(t):\n",
    "#     for i in dfs:\n",
    "#         foo = i+\".head()\"\n",
    "#         exec(foo) # => TableName_df\n",
    "\n",
    "# for df in dfs:\n",
    "#     print(exec(df))\n",
    "    \n",
    "# print(dfs)\n",
    "\n",
    "# len(dfs)\n",
    "# x = 0\n",
    "# while x <= len(dfs):\n",
    "#     for i in x:\n",
    "#         foo = i+\".head()\"\n",
    "#         print(foo)\n",
    "#     x=x+1\n",
    "\n",
    "# tbl=[]\n",
    "# for i in dfs:\n",
    "#     foo = i+\".head()\"\n",
    "#     exec(foo) # => TableName_df\n",
    "#     tbl.append(foo)\n",
    "#     print(foo)\n",
    "\n",
    "\n",
    "# tbl = []\n",
    "# for i in dfs:\n",
    "#     t = str(i+\".head()\")\n",
    "#     test = t\n",
    "#     test=str(test)\n",
    "#     print(test)\n",
    "#     display(test)\n",
    "#     exec(test)\n",
    "#     tbl.append(test)\n",
    "\n",
    "# for x in tables:\n",
    "#     x\n",
    "# dfEmployee.head()\n",
    "# Employee\n",
    "# tbl (dfs)\n",
    "# tbl(dfs)\n",
    "# dfEmployee.head()\n",
    "# x\n",
    "# dfEmployee.head()\n",
    "\n",
    "# def foo (t):\n",
    "#     for i in tbl:\n",
    "#         eval (i)\n",
    "        \n",
    "# foo(tbl)\n",
    "# tables\n",
    "# def u (tq=tables):\n",
    "#     for t in tq:\n",
    "#         ldic=locals()\n",
    "#         foo = i+\".head()\"\n",
    "#         print(foo)\n",
    "# #         exec(foo) # => TableName_df\n",
    "\n",
    "    \n",
    "# def test3():\n",
    "#     d=1\n",
    "#     ldic=locals()\n",
    "#     exec(\"d+=1\",globals(),ldic)\n",
    "#     d=ldic[\"d\"]\n",
    "#     print(d) # it works! returns 2\n",
    "# tables    \n",
    "# u(tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "The goal of this task is to examine the data within all datasets more closely. We will look at the range of values for each variable and their distributions. This should allow us to get familiar with data, spot signs of data quality problems and set the stage for data preperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-aa22c93da2d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mast_node_interactivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# dfEmployee.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "# Return the first 5 rows for each dataset:\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "eval(dfs)\n",
    "\n",
    "# dfEmployee.head()\n",
    "# dfCategory.head()\n",
    "# dfCustomer.head()\n",
    "# dfShipper.head()\n",
    "# dfSupplier.head()\n",
    "# dfOrder.head()\n",
    "# dfProduct.head()\n",
    "# dfOrderDetail.head()\n",
    "# dfCustomerCustomerDemo.head()\n",
    "# dfCustomerDemographic.head()\n",
    "# dfRegion.head()\n",
    "# dfTerritory.head()\n",
    "# dfEmployeeTerritory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics that summarize the central tendency,\n",
    "# dispersion and shape of a dataset's distribution, excluding \"NaN\" values:\n",
    "\n",
    "dfEmployee.describe()\n",
    "dfCategory.describe()\n",
    "dfCustomer.describe()\n",
    "dfShipper.describe()\n",
    "dfSupplier.describe()\n",
    "dfOrder.describe()\n",
    "dfProduct.describe()\n",
    "dfOrderDetail.describe()\n",
    "dfCustomerCustomerDemo.describe()\n",
    "dfCustomerDemographic.describe()\n",
    "dfRegion.describe()\n",
    "dfTerritory.describe()\n",
    "dfEmployeeTerritory.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the dataset including the index, column dtypes, non-null values \n",
    "# and memory usage:\n",
    "\n",
    "print('Table: dfEmployee: ')\n",
    "print('===================')\n",
    "dfEmployee.info()\n",
    "print('')\n",
    "print('Table: dfCategory: ')\n",
    "print('===================')\n",
    "dfCategory.info()\n",
    "print('')\n",
    "print('Table: dfCustomer: ')\n",
    "print('===================')\n",
    "dfCustomer.info()\n",
    "print('')\n",
    "print('Table: dfShipper: ')\n",
    "print('===================')\n",
    "dfShipper.info()\n",
    "print('')\n",
    "print('Table: dfSupplier: ')\n",
    "print('===================')\n",
    "dfSupplier.info()\n",
    "print('')\n",
    "print('Table: dfOrder: ')\n",
    "print('===================')\n",
    "dfOrder.info()\n",
    "print('')\n",
    "print('Table: dfProduct: ')\n",
    "print('===================')\n",
    "dfProduct.info()\n",
    "print('')\n",
    "print('Table: dfOrderDetail: ')\n",
    "print('===================')\n",
    "dfOrderDetail.info()\n",
    "print('')\n",
    "print('Table: dfCustomerCustomerDemo: ')\n",
    "print('===============================')\n",
    "dfCustomerCustomerDemo.info()\n",
    "print('')\n",
    "print('Table: dfCustomerDemographic: ')\n",
    "print('==============================')\n",
    "dfCustomerDemographic.info()\n",
    "print('')\n",
    "print('Table: dfRegion: ')\n",
    "print('===================')\n",
    "dfRegion.info()\n",
    "print('')\n",
    "print('Table: dfTerritory: ')\n",
    "print('===================')\n",
    "dfTerritory.info()\n",
    "print('')\n",
    "print('Table: dfEmployeeTerritory: ')\n",
    "print('============================')\n",
    "dfEmployeeTerritory.info()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmployee # 9 records; no values for Photo column, 1 record has no ReportsTo value\n",
    "dfCategory # 8 records; no null values\n",
    "dfCustomer # 91 records; 1 record where Postal Code is null, 22 records where Fax is null\n",
    "dfShipper  # 3 records; no null values\n",
    "dfSupplier # 29 records; 16 records where Fax is null, 24 records where HomePage is null\n",
    "dfOrder    # 830 records; 21 records where ShippedDate is null, 19 records where Ship Postal Code is null\n",
    "dfProduct  # 77 records; no null values\n",
    "dfOrderDetail  # 2155 records; no null values\n",
    "dfCustomerCustomerDemo  # 0 records\n",
    "dfCustomerDemographic   # 0 records\n",
    "dfRegion  # 4 records; no null values\n",
    "dfTerritory  # 53 records; no null values\n",
    "dfEmployeeTerritory  # 49 records; no null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the dimensionality of the datasets:\n",
    "\n",
    "print('dfEmployee Shape: ',dfEmployee.shape)\n",
    "print('dfCategory Shape: ',dfCategory.shape)\n",
    "print('dfCustomer Shape: ',dfCustomer.shape)\n",
    "print('dfShipper Shape: ',dfShipper.shape)\n",
    "print('dfSupplier Shape: ',dfSupplier.shape)\n",
    "print('dfOrder Shape: ',dfOrder.shape)\n",
    "print('dfProduct Shape: ',dfProduct.shape)\n",
    "print('dfOrderDetail Shape: ',dfOrderDetail.shape)\n",
    "print('dfCustomerCustomerDemo Shape: ',dfCustomerCustomerDemo.shape)\n",
    "print('dfCustomerDemographic Shape: ',dfCustomerDemographic.shape)\n",
    "print('dfRegion Shape: ',dfRegion.shape)\n",
    "print('dfTerritory Shape: ',dfTerritory.shape)\n",
    "print('dfEmployeeTerritory Shape: ',dfEmployeeTerritory.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Verify Data Quality\n",
    "During this portion of the analysis, I derived the following:\n",
    "\n",
    "* Employee table: 9 total rows; 1 row has a NaN Reports To value\n",
    "* Customer table: 91 total rows; 1 row has a NaN Postal Code value, 22 rows have NaN Fax values\n",
    "* Supplier table: 29 total rows; 16 rows have NaN Fax values, 24 rows have NaN Home Page values\n",
    "* Order table: 830 total rows; 21 rows have NaN Shipped Date values, 19 rows have NaN Ship Postal Code values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation phase covers all activities to construct the final datasets from the initial raw data. These activities include the following:\n",
    "\n",
    "* Select Data\n",
    "* Clean Data\n",
    "* Construct Data\n",
    "* Integrate Data\n",
    "* Format Data\n",
    "\n",
    "Data preparation is 80% of the process. The two core activities in this phase are \n",
    "Data Wrangling and Data Analysis; they are the first logical programming steps. Data Wrangling is cyclical in nature and is language/framwork independent, so it will be necessary revisit the steps multiple times.\n",
    "\n",
    "We will perform syntactical and meaningful checks on the data and identify any issues and recommend potential fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfOrder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data\n",
    "This task invoves the follwing steps:\n",
    "\n",
    "* Check for missing data/impute values\n",
    "* Check for duplicates\n",
    "* Check for extraneous values\n",
    "* Drop columns (if necessary)\n",
    "* Drop rows (if necessary)\n",
    "\n",
    "First, we will check for missing data by executing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dfEmployee Columns Missing Data: ')\n",
    "print('================================')\n",
    "print(dfEmployee.isna().any())\n",
    "print('')\n",
    "print('dfCustomer Columns Missing Data: ')\n",
    "print('================================')\n",
    "print(dfCustomer.isna().any())\n",
    "print('')\n",
    "print('dfSupplier Column Missing Data: ')\n",
    "print('===============================')\n",
    "print(dfSupplier.isna().any())\n",
    "print('')\n",
    "print('dfOrder Columns Missing Data: ')\n",
    "print('=============================')\n",
    "print(dfOrder.isna().any())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counts for missing values in columns:\n",
    "\n",
    "print('dfEmployee Missing Counts: ')\n",
    "print('==========================')\n",
    "print(dfEmployee.isna().sum())\n",
    "print('')\n",
    "print('dfCustomer Missing Counts: ')\n",
    "print('==========================')\n",
    "print(dfCustomer.isna().sum())\n",
    "print('')\n",
    "print('dfSupplier Missing Counts: ')\n",
    "print('==========================')\n",
    "print(dfSupplier.isna().sum())\n",
    "print('')\n",
    "print('dfOrder Missing Counts: ')\n",
    "print('=======================')\n",
    "print(dfOrder.isna().sum())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inquiries have confirmed that null values exist, so now we will determine the percentage of null values within the columns and view the unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmployee.Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dfEmployee)\n",
    "len(dfEmployee[dfEmployee.Photo.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what percentage of rows in the dfEmployee Photo and ReportsTo columns contain missing values\n",
    "# Print out the number of unique values in this column\n",
    "\n",
    "print('Percentage of Null Photo Values:', round((len(dfEmployee[dfEmployee.Photo.isna()])/ len(dfEmployee)),2)*100,'%')\n",
    "print('Number of Unique Photo Values Excluding Nulls:', dfEmployee.Photo.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('dfEmployee Photo Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfEmployee.Photo.value_counts())\n",
    "print (' ')\n",
    "\n",
    "print('Percentage of Null ReportsTo Values:', round((len(dfEmployee[dfEmployee.ReportsTo.isna()])/ len(dfEmployee)),2)*100,'%')\n",
    "print('Number of Unique ReportsTo Values Excluding Nulls:', dfEmployee.ReportsTo.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('dfEmployee ReportsTo Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfEmployee.ReportsTo.value_counts())\n",
    "print (' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what percentage of rows in the dfCustomer PostalCode and Fax columns contain missing values\n",
    "# Print out the number of unique values in this column\n",
    "\n",
    "print('Percentage of Null Postal Code Values:', round((len(dfCustomer[dfCustomer.PostalCode.isna()])/ len(dfCustomer)),2)*100,'%')\n",
    "print('Number of Unique Photo Values Excluding Nulls:', dfCustomer.PostalCode.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('Photo Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfCustomer.PostalCode.value_counts())\n",
    "print (' ')\n",
    "\n",
    "print('Percentage of Null Fax Values:', round((len(dfCustomer[dfCustomer.Fax.isna()])/ len(dfCustomer)),2)*100,'%')\n",
    "print('Number of Unique Fax Values Excluding Nulls:', dfCustomer.Fax.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('Fax Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfCustomer.Fax.value_counts())\n",
    "print (' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what percentage of rows in the dfSupplier Fax and HomePage columns contain missing values\n",
    "# Print out the number of unique values in this column\n",
    "\n",
    "print('Percentage of Null Fax Values:', round((len(dfSupplier[dfSupplier.Fax.isna()])/ len(dfSupplier)),2)*100,'%')\n",
    "print('Number of Unique Fax Values Excluding Nulls:', dfSupplier.Fax.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('Fax Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfSupplier.Fax.value_counts())\n",
    "print (' ')\n",
    "\n",
    "print('Percentage of Null HomePage Values:', round((len(dfSupplier[dfSupplier.HomePage.isna()])/ len(dfSupplier)),2)*100,'%')\n",
    "print('Number of Unique HomePage Values Excluding Nulls:', dfSupplier.HomePage.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('HomePage Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfSupplier.HomePage.value_counts())\n",
    "print (' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what percentage of rows in the dfOrder ShippedDate and ShipPostalCode columns contain missing values\n",
    "# Print out the number of unique values in this column\n",
    "\n",
    "print('Percentage of Null ShippedDate Values:', round((len(dfOrder[dfOrder.ShippedDate.isna()])/ len(dfOrder)),2)*100,'%')\n",
    "print('Number of Unique ShippedDate Values Excluding Nulls:', dfOrder.ShippedDate.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('ShippedDate Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfOrder.ShippedDate.value_counts())\n",
    "print (' ')\n",
    "\n",
    "print('Percentage of Null ShipPostalCode Values:', round((len(dfOrder[dfOrder.ShipPostalCode.isna()])/ len(dfSupplier)),2)*100,'%')\n",
    "print('Number of Unique ShipPostalCode Values Excluding Nulls:', dfOrder.ShipPostalCode.nunique())\n",
    "# print (len(df.waterfront.unique()))\n",
    "print (' ')\n",
    "print ('ShipPostalCode Counts')\n",
    "print ('------------------')\n",
    "# print(len(dfEmployee.Photo.unique()))\n",
    "print(dfOrder.ShipPostalCode.value_counts())\n",
    "print (' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Data\n",
    "\n",
    "There is no need to add new fields/rows any of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating Data\n",
    "\n",
    "We did not have any disparate datasets to add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting Data\n",
    "\n",
    "There is no need to format the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to ask some questions before proceeding. The questions are as follows:\n",
    "\n",
    "* Does it make sense to drop columns containing NaN values from the datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the results above, we concluded that we can remove the following columns from the datasets:\n",
    "\n",
    "* dfEmployee.Photo\n",
    "* dfCustomer.Fax\n",
    "* dfSupplier.Fax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfEmployee # (Photo (9), ReportsTo (1))\n",
    "# dfCustomer # (PostalCode (1), Fax (22))\n",
    "# dfSupplier # (Fax (16), HomePage (24))\n",
    "# dfOrder    # (ShippedDate (21), ShipPostalCode (19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specified labels from columns:\n",
    "\n",
    "def drop_cols(columns, df):\n",
    "    return df.drop(columns, axis=1, inplace=True)\n",
    "\n",
    "drop_cols(['Photo'], dfEmployee)\n",
    "drop_cols([\"Fax\"], dfCustomer)\n",
    "drop_cols([\"Fax\"], dfSupplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the methodology, modeling techniques are now selected and applied at this point during the process.  Since some techniques have specific requirements regarding the structure of the data, so there can be a loop back to Data Preparation. The tasks are:\n",
    "\n",
    "* Select Modeling Technique\n",
    "* Generate Test Design\n",
    "* Build Model\n",
    "* Assess Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Modeling Technique\n",
    "\n",
    "The modeling technique that we will use is dependent upon each question posed in the Business Goals section.\n",
    "\n",
    "1.) Does discount amount have a statistically significant effect on the quantity of a product in an order? If so, at what level(s) of discount?\n",
    "\n",
    "To answer this question, we will need to understand the distribution of product quantities associated with discount percentages within the Order Detail dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provide counts of records based on discounts:\n",
    "\n",
    "dfOrderDetail.Discount.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the insignificant counts related to the discounts of 1%,2%,3%,4% and 6%, we will drop these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list containing relelvant discount totals:\n",
    "\n",
    "newdisc = (dfOrderDetail.Discount == 0.00) | (dfOrderDetail.Discount == 0.05) | (dfOrderDetail.Discount == 0.10) | (dfOrderDetail.Discount == 0.15) | (dfOrderDetail.Discount == 0.20) | (dfOrderDetail.Discount == 0.25)\n",
    "dfdisc = dfOrderDetail.loc[newdisc]\n",
    "dfdisc.Discount.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram to view values:\n",
    "\n",
    "dfdisc.Discount.hist(figsize=(6, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create boxplots to view outliers:\n",
    "\n",
    "dfdisc.boxplot('Quantity', by='Discount', figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfdisc.loc[dfdisc['Discount']==.15].sort_values('Quantity',ascending = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a representation of the distribution of data for each variable:\n",
    "\n",
    "dfdisc.hist('Quantity', by='Discount', figsize=(10, 10));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create discount and non-discount lists for the means:\n",
    "\n",
    "disc = dfOrderDetail[dfOrderDetail['Discount']!=0].groupby('ProductId')['Quantity'].mean()\n",
    "no_disc = dfOrderDetail[dfOrderDetail['Discount']==0].groupby('ProductId')['Quantity'].mean()\n",
    "\n",
    "sns.distplot( no_disc , color=\"skyblue\", label=\"Non-discounted Orders\")\n",
    "sns.distplot( disc , color=\"darkblue\", label=\"Discounted Orders\")\n",
    "plt.legend()\n",
    " \n",
    "plt.show()\n",
    "print(\"Average number of items ordered with discount is {}\".format(round(disc.values.mean(),2))) \n",
    "print(\"Average number of items ordered without discount is {}\".format(round(no_disc.values.mean(),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Shapiro-Wilk test tests the null hypothesis that the\n",
    "# data was drawn from a normal distribution\n",
    "\n",
    "# Confirm normality with non-discounted data:\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "no_disc = np.array(no_disc)\n",
    "mu = np.mean(no_disc)\n",
    "sig = np.std(no_disc)\n",
    "no_disc = np.random.normal(mu, sig, 1317)\n",
    "stat, p = shapiro(no_disc)  # Perform the Shapiro-Wilk test for normality\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "alpha = .05\n",
    "if p > alpha:\n",
    "    print('Non-discounted data looks normal')\n",
    "else:\n",
    "    print('Non-discounted data does not look normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm normality with discounted data:\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "disc = np.array(disc)\n",
    "mu = np.mean(disc)\n",
    "sig = np.std(disc)\n",
    "disc = np.random.normal(mu, sig, 1317)\n",
    "stat, p = shapiro(disc)  # Perform the Shapiro-Wilk test for normality\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "alpha = .05\n",
    "if p > alpha:\n",
    "    print('Non-discounted data looks normal')\n",
    "else:\n",
    "    print('Non-discounted data does not look normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if the sample observations have numeric and continuous values. It also appears that the sample observations are independent from each other (that is, that you have a simple random sample) and that the samples have been drawn from normal distributions, so this scenario is a candidate for a t-test.\n",
    "\n",
    "In addition, when two groups have equal sample sizes and variances, Welch‚Äôs t-test tends to give the same result as a t-test. However, when sample sizes and variances are unequal, the t-test is unreliable, whereas Welch‚Äôs tends perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Test Design "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welch's t-test is more robust than Student's t-test and maintains Type I error rates close to nominal for unequal variances and for unequal sample sizes under normality. Furthermore, the power of Welch's t-test comes close to that of Student's t-test, even when the population variances are equal and sample sizes are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohen‚Äôs d is one of the most common ways to measure effect size. As an effect size, Cohen's d is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n",
    "\n",
    "The general ‚Äúrule of thumb‚Äù guidelines for Cohen's d is as follows:\n",
    "\n",
    "* Small effect = 0.2\n",
    "\n",
    "* Medium Effect = 0.5\n",
    "\n",
    "* Large Effect = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 5 steps to execute the Welch's t-test:\n",
    "\n",
    "* Set up null and alternative hypotheses\n",
    "* Choose a significance level\n",
    "* Set up Cohen's d function to determine effect\n",
    "* Conduct the Welch's t-test\n",
    "* Determine the p-value (find the rejection region)\n",
    "* Accept or reject the Null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Set up null and alternative hypotheses\n",
    "\n",
    "$H_O$: Discounts have no effect on the number of products customers order.<br> \n",
    "$H_a$: Discounts have an effect on the number of products customers order.<br> \n",
    "\n",
    "\n",
    "##### * Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Cohen's d function:\n",
    "\n",
    "def Cohen_d(group1, group2):\n",
    "    diff = group1.mean() - group2.mean()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    return abs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Conduct the Welch's t-test\n",
    "##### * Determine the p-value\n",
    "##### * Accept or reject the Null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct Welch's t- test:\n",
    "\n",
    "from scipy import stats \n",
    "no_disc = dfdisc[dfdisc['Discount']==0]['Quantity']  # control group\n",
    "disc = dfdisc[dfdisc['Discount']!=0]['Quantity']     # experimental group  \n",
    "\n",
    "'''It is important to keep the discounted items and non-discounted items seperate. We will\n",
    "refer to the non-discounted items as the control. This prevents any undue influence.'''\n",
    "\n",
    "t_stat, p = stats.ttest_ind(no_disc, disc)   # welchs t-test \n",
    "d = Cohen_d(disc, no_disc)\n",
    "print('p_value =', p)\n",
    "print('Reject null hypothesis') if p < 0.05 else print('Failed to reject null hypothesis')\n",
    "print(\"Cohen's d =\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Model\n",
    "Since the p-value is less than the selected significance level, we can reject the null hypothesis.\n",
    "In addition, the Cohen's d value suggests that effect of discounts on order quantities is small in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is there a statistically significant difference in discount between Categories?\n",
    "\n",
    "* Is there a statistically significant difference in performance of Shipping Companies?\n",
    "\n",
    "* Is there a statistically significant difference in performance of Suppliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer these questions, we will use ANOVA (Analysis of Variance) testing. ANOVA is a method for generalizing statistical tests to multiple groups. ANOVA analyses the overall variance of a dataset by partitioning the total sum of square of deviations (from the mean) into the sum of squares for each of these groups and sum of squares for error. By comparing the statistical test for multiple groups, it can serve as a useful alternative to ùë° -tests when testing multiple factors simultaneously is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-way ANOVA is an extension of the one-way ANOVA that examines the influence of two different categorical independent variables on one continuous dependent variable. The two-way ANOVA not only aims to assess the main effect of each independent variable but also to see if there is any interaction between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.) Discounts Between Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up null and alternative hypotheses\n",
    "\n",
    "- $H_0$: There is no difference in discount levels between categories\n",
    "- $H_\\alpha$: There is a difference in discount levels between categories\n",
    "\n",
    "\n",
    "##### Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catdisc = pd.read_sql_query('''\n",
    "SELECT OrderDetail.UnitPrice, Discount, CategoryId \n",
    "FROM OrderDetail\n",
    "JOIN Product\n",
    "ON OrderDetail.ProductId = Product.Id\n",
    "''',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols = Create a Model from a formula and dataframe\n",
    "\n",
    "formula = 'Discount ~ C(CategoryId)'  # The formula specifying the model\n",
    "lm = ols(formula, catdisc).fit()   # The data for the model\n",
    "table = sm.stats.anova_lm(lm, typ=2) \n",
    "# Anova table for one or more fitted linear models\n",
    "# lm = model, typ=2 indicates type of ANOVA test\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANOVA Definitions:\n",
    "* sum_sq = the sum of squares due to the source\n",
    "* df = the degrees of freedom in the source\n",
    "* F = the F-statistic; variance of the group means (Mean Square Between) / mean of the within group variances (Mean Squared Error)\n",
    "* PR(>F) = probability of getting a given F-statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that there is no _**statistically significant**_ difference in discount level between Categories, therefore we cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.)  Shipping Companies' Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up null and alternative hypotheses\n",
    "\n",
    "- $H_0$: There is no difference in performance of Shipping Companies\n",
    "- $H_\\alpha$: There is a difference in performance of Shipping Companies\n",
    "\n",
    "\n",
    "##### Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert argument to datetime:\n",
    "\n",
    "dfOrder.OrderDate = pd.to_datetime(dfOrder.OrderDate)\n",
    "dfOrder.ShippedDate = pd.to_datetime(dfOrder.ShippedDate)\n",
    "dfOrder.RequiredDate = pd.to_datetime(dfOrder.RequiredDate)\n",
    "\n",
    "# Calculate Processing and Shipping Time:\n",
    "\n",
    "dfOrder['ProcessingTime'] = dfOrder.ShippedDate - dfOrder.OrderDate\n",
    "dfOrder['ShippingTime'] = dfOrder.RequiredDate - dfOrder.ShippedDate\n",
    "\n",
    "# Convert to number of days:\n",
    "\n",
    "dfOrder.ShippingTime = dfOrder.ShippingTime.dt.days\n",
    "dfOrder.ProcessingTime = dfOrder.ProcessingTime.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View results of table:\n",
    "\n",
    "dfOrder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the mean for all values:\n",
    "\n",
    "dfOrder.groupby('ShipVia').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'ProcessingTime ~ C(ShipVia)'\n",
    "lm = ols(formula, dfOrder).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that there is a _**statistically significant**_ difference in Shipping Compnaies, therefore we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.)  Supplier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up null and alternative hypotheses\n",
    "\n",
    "- $H_0$: There is no difference in performance of Suppliers\n",
    "- $H_\\alpha$: There is a difference in performance of Suppliers\n",
    "\n",
    "\n",
    "##### Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = pd.read_sql_query(\"\"\"\n",
    "SELECT CompanyName, a.Quantity\n",
    "FROM OrderDetail a\n",
    "JOIN Product b ON a.ProductID = b.ID\n",
    "JOIN Supplier c ON b.SupplierID = c.ID\n",
    "\"\"\",conn)\n",
    "# 51317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'Quantity ~ C(CompanyName)'\n",
    "lm = ols(formula, reg).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that there is no _**statistically significant**_ difference in Supplier performance, therefore we cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
